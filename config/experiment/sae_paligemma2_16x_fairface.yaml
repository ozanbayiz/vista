# @package _global_

# Experiment: Train 16x BatchTopK SAE on PaliGemma2 FairFace latents
# Usage: python -m src.main +experiment=sae_paligemma2_16x_fairface

defaults:
  - override /model: sae_paligemma2_16x
  - override /data: fairface_paligemma2_no_labels
  - override /module: sae

trainer:
  max_epochs: 10
  precision: 16-mixed

wandb:
  project: idarve
  enabled: true
