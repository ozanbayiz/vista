\begin{abstract}
Sparse autoencoders (SAEs) are increasingly used to identify and suppress demographic features in vision encoders, with reported fairness improvements on classification and retrieval benchmarks. We show that on free-form generation tasks, these effects are confounded by a \emph{reconstruction-error confound}: the SAE's encode-decode process itself---without modifying any features---accounts for a substantial, architecture-dependent portion of the observed demographic content reduction. Through a six-method decomposition (noise, SAE passthrough, random suppression, targeted SDF suppression, LEACE, and S\&P Top-K projection) across three VLMs with distinct vision encoder architectures (PaliGemma2, Qwen2-VL, Qwen3-VL), we separate SAE-based intervention effects into generic perturbation ($\sim$4\,pp), structured reconstruction error (0--16\,pp, architecture-dependent), and genuine targeted suppression ($\sim$12\,pp). The reconstruction component scales monotonically with SAE sparsity and varies dramatically across models: it dominates on PaliGemma2 (83\% baseline gender mention rate), vanishes on Qwen3-VL, and produces \emph{opposite-direction} effects on Qwen2-VL where suppression \emph{increases} demographic content by converting neutral language into gendered hallucinations. We further show that LEACE, a linear concept erasure method that bypasses the SAE reconstruction path, reveals a VQA--captioning dissociation: VQA accuracy drops to chance (50\%) under gender erasure while captioning is nearly unaffected, indicating that these tasks access demographic information through different mechanisms. All results are validated with LLM-as-judge evaluation ($n{=}1{,}000$, bootstrap CIs), which also exposes a 77--98\% false-positive rate in keyword-based race metrics. Our decomposition framework provides a necessary diagnostic for any SAE-based intervention study.
\end{abstract}
