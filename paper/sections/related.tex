\section{Related Work}
\label{sec:related}

\paragraph{Sparse autoencoders for interpretability.}
SAEs decompose neural network activations into sparse, monosemantic features by training an overcomplete dictionary with a sparsity penalty~\citep{cunningham2023sparse, bricken2023monosemanticity}. BatchTopK SAEs~\citep{bussmann2024batchtopk} replace the $L_1$ penalty with a deterministic top-$k$ selection, offering better reconstruction--sparsity trade-offs. Recent work extends SAEs to vision transformers~\citep{stevens2025interpretable}, CLIP encoders~\citep{joseph2025steering}, and the vision towers of VLMs~\citep{pach2025saes}. Matryoshka SAEs~\citep{zaigrajew2025matryoshka} learn hierarchical features at multiple granularities, achieving near-perfect reconstruction (0.99 cosine similarity on CLIP). SAEBench~\citep{karvonen2025saebench} provides a comprehensive evaluation suite for SAEs, revealing that proxy metrics (e.g., reconstruction quality) do not reliably predict practical effectiveness. Our work builds on this foundation but critically examines the confound introduced by SAE reconstruction error during downstream VLM generation.

\paragraph{SAE-based VLM debiasing.}
\citet{sasse2024debias} train SAEs on CLIP-family vision encoders across five VLMs and suppress demographic features, reporting 5--15 point fairness improvements on classification, VQA, and captioning benchmarks. \citet{barbalau2025select} propose Select \& Project (S\&P) Top-K, which uses the SAE encoder weights to define a concept direction for orthogonal projection, reporting up to 3.2$\times$ fairness gains over standard SAE suppression on CelebA and FairFace classification tasks. Neither work reports passthrough baselines that would isolate reconstruction error from targeted suppression. We show that when evaluated on free-form captioning with passthrough controls, S\&P Top-K has negligible downstream impact ($<$0.5\pp), and standard SAE suppression effects are substantially inflated by reconstruction error on certain architectures.

\paragraph{Linear concept erasure.}
LEACE (Least-squares Concept Erasure;~\citealt{belrose2023leace}) removes concept information via an optimal linear projection that guarantees no linear probe can recover the erased attribute. SPLINCE~\citep{holstege2025splince} extends this framework by preserving task-relevant covariance while removing concepts. Unlike SAE-based methods, these projection methods introduce no reconstruction bottleneck---the projection preserves $>$99.9\% of the activation variance. We use LEACE as a bottleneck-free baseline to isolate genuinely targeted effects from the SAE reconstruction confound.

\paragraph{VLM fairness and debiasing.}
A growing literature addresses demographic bias in VLMs through training-time interventions~\citep{zhang2025joint}, test-time representation editing~\citep{bendvlm2024}, feature pruning~\citep{sfid2024}, and contrastive projection~\citep{prism2025}. These methods typically evaluate on benchmark classification tasks. Our contribution is orthogonal: we provide a diagnostic framework for \emph{any} intervention that modifies vision encoder activations, decomposing its effect into bottleneck and targeted components.

\paragraph{Evaluation of demographic content.}
Keyword-based demographic content rate (DCR) metrics count occurrences of demographic terms in generated text~\citep{zhao2017men}. We show this approach has severe limitations for racial attributes due to polysemous colour terms, and complement it with LLM-as-judge evaluation using both binary classification (Qwen3-8B;~\citealt{yang2025qwen3}) and pairwise comparison (JudgeLRM-7B;~\citealt{judgelrm2025}).
